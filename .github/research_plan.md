## 1. 연구 개요

### 1. 제목

1. I **Don't Know the Law, but I Can Solve the Problem: Pitfalls in Evaluating Legal Knowledge.**
2. **I Don't Know the Law, but I Can Solve the Problem: Inverse Potemkin Understanding in Legal Knowledge Evaluation**
- 기타 후보들
    1. LLM 법규 지식 평가의 이중 함정: '역 포템킨 이해'와 '객관식 지름길' 현상 분석 및 검증 체계 제안
    2. I Don't Know the Law, but I Can Solve the Problem: Evaluating the Traffic Law Knowledge of LLMs for Autonomous Driving.
    3. I **Don't Know the Law, but I Can Solve the Problem: Pitfalls in Evaluating Legal Knowledge, from 'Inverse Potemkin Understanding' to Test-Taking Shortcuts.**
        1. "I Don't Know the Law, but I Can Solve the Problem: Inverse Potemkin Understanding in Legal Knowledge Evaluation”
    4. **The Pitfall of Evaluating Legal Knowledge in LLMs: Measuring 'Inverse Potemkin Understanding' and Proposing a Reasoning-Based Verification Framework.**
    5. **The Inverse Potemkin Problem: How LLMs Solve Legal Cases without Legal Knowledge**

### 2. 연구목적 및  개요

1. **문제 제기** : LLM이 실제 지식 없이도 특정 과업을 해결할 수 있는 것처럼 보이는 문제를 '역 포템킨 이해(IPU)'로 정의하고 법규지식 평가에서 이 현상을 검증한다. 또한 문제형식이 역 포템킨 이해 현상에 어떤 영향을 미치는지 실증적으로 분석한다.
2. **제안** : 단순히 객관식 문제의 정답을 맞추는 것만(what)으로 법규지식을 평가해서는 안되며 원하는 방식(how, 여기서는 법적 근거 기반)으로 정답에 도달했는지를 함께 평가하는 검증체계가 필요함. 그 예시로서 정답 추론의 근거를 제시하도록 하는 방법을 제안해 검증체계 방법론 가이드라인 제공

## 2. 연구의 배경(연구문제_problem setup)

### **1. 문제 제기**

- **교통경찰업무, 자율주행 관점에서 정확한 법규지식 평가의 중요성**
    - LLM을 교통의 관점에서 차량의 판단 및 제어시스템에 통합하려는 시도, 교통 경찰 업무에 LLM을 활용하려는 시도가 존재. 활용의 안전성을 보장하기 위해서는 LLM이 도로교통법과 같은 교통법규 지식 이해 능력을 신뢰성있게 평가하는 것이 중요.
- **역포템킨 현상과 위험성**
    - '역 포템킨 이해(Inverse Potemkin Understanding)' 현상
        - 관련(근거) 논문
            - 포템킨 이해 현상 : 개념은 알지만 적용을 못하는 걸 말한다. [Potemkin Understanding in Large Language Models | OpenReview](https://www.notion.so/Potemkin-Understanding-in-Large-Language-Models-OpenReview-221e0712d28581f4a6ffd0c3310a946d?pvs=21)
            - [대규모 언어모델 기반 프롬프트를 활용한 운전면허 학과시험 문항 분석 프레임워크 개발](https://www.notion.so/225e0712d28581dcb9ffc1a0dd4f791e?pvs=21) : 사진, 일러스트 처럼 사례해결형 문제에서 문장형 문제보다 상대적으로 높은 정답율 기록(다만 동영상 문제에서는 좋지 않은 점수를 얻었고 저자는 복합판단의 어려움으로 해석)
        - 정의
            - LLM의 법규 지식 평가에서 정답률(ACC)은 높게 나타나지만, 그 정답의 법적 근거 정확도(LRA)는 현저히 낮게 나타나는 '유의미한 괴리(discrepancy)' 현상으로 정의한다. 포템킨 이해 현상과 비교해 적용은 잘 하지만 실제로 개념(법률)은 모르는 현상이라 할 수 있다.
    - 도로교통법규 평가에서 역포템킨현상의 가능성과 위험성
        - 가능성 : 운전과 보행은 일상적이므로, 일상의 경험, 상식적 판단이 법적 판단과 관련성이 높다. LLM이 교통법규 지식 없이도 MCQA에서 높은 평가 점수를 받는 역  포템킨 현상의 가능성이 존재한다.
        - 위험성 : 법규지식 및 이해 능력이 과대평가되어 정작 현실에서 문제를 일으킬 소지 존재
- **문제형식과 역포템킨 현상**
    - 문장형과 사진/일러스트형
        - 법규지식이 아닌 선택지간의 관계, 상식, 일상의 경험을 이용한다면 직접적으로 단순하게 법적 지식을 묻는 문장형보다는 실제 일상과 유사한 사진/일러스트형 문제에서 높은 점수를 받을 수 있음
        - 만약 맞다면 법규지식은 문장형으로 묻는 것도 필요
    - 객관식과 오픈형
        - 객관식 지름길 현상
            - 관련(근거) 논문 및 의미
                - [Answer Matching Outperforms Multiple Choice for Language Model Evaluation] _ (zotero://select/library/items/Z4WF5D4L) : 객관식은 판별문제이며 선택지만으로 정답을 고르는 지름길이 존재함으로 평가에 부적합. 객관식 정답은 맞췄지만 오픈형으로 풀었을 때는 매우 오답율이 높고 일치도는 26%에 불과해 LLM의 실제 문제해결능력은 부족한데 객관식만을 잘푸는 평가의 타당성 문제 지적.
            - 내용
                - 법규지식을 직접적으로 평가하는 객관식 문장형 문제조차도 법규 지식이 아니라 선택지간의 관계 혹은 다른 방법으로 정답을 판별하는 가능성이 존재하며 이는 법규 지식이 아닌 문제 형식의 허점을 이용한 결과이며 역시 역포템킨 이해 현상의 하나로 볼 수 있음
        - 문제형식(객관식, 오픈형)이 정답율과 법적근거 일치도의 차이에 영향을 미치는지 확인 필요 → 만약 객관식이 낮다면 문장형으로도 부족함.
- 객관식 지름길과 역포템킨 이해의 관계 명확화
    - 구체적으로는 역포템킨 이해 현상이 문제형식에 따라 어떻게 나타나는가를 보는 것이 목적이고, 그 원인으로 객관식 지름길 현상을 하나로 볼 수 있음. 객관식 지름길 현상(객관식) 혹은 상식 의존(사진/일러스트형)은 해석적 틀임.

### **2. 제안**

- 이러한 가능성들은 LLM이 정답을 맞혔다는 결과(what)를 검증하는 단순 MCQA방식만으로는 그 능력치를 신뢰하기 어렵게 만드는 요인이 될 수 있다. 따라서 본 연구는 LLM의 법규 지식을 올바르게 평가하기 위해, 정답에 도달하는 방식(how)의 타당성, 즉 **'법적 근거에 기반한 추론 능력'을 직접 검증하는 별도의 체계가 필수적임을 주장**하고 그 구체적인 방법론을 제안.
- 좋은 성능이란, 답변과 답변에 이르는 방식이 모두 정확해야함. 즉, 정답을 맞췄는지와 함께 **원하는 방식(법적 근거 기반)으로 정답에 도달했는지(how)**도 평가(법적 근거 기반 vs 상식적 판단 기반)
- 따라서 MCQA를 쓰되, 법적 근거를 함께 제시하는 방식을 제안(오픈형은 해당 방식으로 출제할 수 있는 문제 유형이 한정적이라는 한계)

## 3. 연구질문(research_question) 및 가설(hypotheses)

### 1. 연구질문

- **RQ1:** LLM의 법규 지식 평가에서,  '역 포템킨 이해' 현상이 나타나는가
- **RQ2:** 문제 형식은 '역 포템킨 이해' 현상에 어떤 영향을 미치는가?

### 2. 연구 가설(Hypotheses)

- **가설 1 :** LLM의 정답률보다 법적 근거 일치도는더 낮을 것이다.
- **가설 2-1 :** 객관식일 때 정답율과 법적 근거 일치도의 차이가 오픈형일 때보다 더 클 것이다.(다를 것이다.)
- **가설 2-2** : 법규지식형 문제에서 단순 문장형 보다 사진/일러스트형 사례 문제에서 정답율과 법적 근거 일치도의 차이가 더 클 것이다. (다를 것이다)

## 4. 연구 방법

### 1. 데이터셋 구축 : 운전면허 학과 시험 문제은행을 기반

- **구축 기준 : 2x2 분류**
    1. 법규지식형 문제와 그 외
    2. 문장형과 사진/일러스트형 문제
    
    |  | 문장형 | 사진/일러스트형 |
    | --- | --- | --- |
    | 법규지식형 | A | B |
    | 그 외 | C | D |
- **분류 기준**
    - 법규지식형 문제와 그외
        - 정답 선택지(문제 유형이 맞는 걸 고르든, 틀린 것을 고르든 정답인 선택지)가 법규에 구체적으로 근거하는 문제
        - 관련 법률 : 도로교통법
        - 기타
            - 법령의 범위 : 법제처 기준 법령으로 검색되는 것(행정규칙, 자치법규, 매뉴얼 제외)
            - 2가지 선택 문제에서 하나만 법령 관련인 경우는 제외 : 법규지식만 평가할 수 없는 문제이기 때문에
            - 그럼에도 애매할 때는 해설을 기준으로 함.
    - 문장형과 사진/일러트스형 : 운전면허 학과시험 문제은행에 기 분류되어 있음
        - 운전면허 학과시험 40문항 구성(참고) :  문장형(4지1답, 17문항, 각 2점), 문장형(4지2답, 4문항, 각 3점), 안전표지형(4지1답, 5문항, 각 2점), 사진 형(5지2답, 6문항, 각 3점), 일러스트형(5지2답, 7문항, 각 3 점), 동영상형(4지1답, 1문항, 5점)
- **문제 변환**
    - 분류된 `문장형 법규 문제`에 대해, 원본 객관식 형태와 내용을 동일하게 유지하되 참/거짓을 판단하는 `O/X 문제` 형태 또는 오픈형 문제를 추가로 생성한다. 변환이 가능한 문제만 처리
- **운전면허 학과시험 문제은행의 장점과 한계**
    - 장점
        1. **표준화된 현실 데이터**: 실제 법규 지식 평가에 사용되는 공식적이고 표준화된 도구
        2. **생태학적 타당성**: 실험실에서 인위적으로 제작된 문제가 아닌, 현실에서 법규 지식을 평가하는 실제 방식
        3. **실용적 적용성**: 연구 결과를 실제 LLM 평가 상황에 직접 적용 가능
    - 한계
        - 다만, 문장형과 사진형 문제의 내용이 완전히 동일하지 않아 순수한 형식 효과를 분리하기 어려운 한계가 있다. 이는 통제된 실험 설계와 현실 타당성 간의 trade-off로, 본 연구에서는 후자를 우선시하였다.

### 2. 실험 설계

- **예비 실험 :  문제 동기부여 및 실험 2-2의 동기 제공(생략 가능)**
    - 법규지식형 중 문장형(A), 사진/일러스트형(B)의 평균 정답률 비교
    - 목적 : 사진/일러스트형(B)이 점수가 더 높게 나온다면 역포템킨 현상을 의심하는 계기가 됨
- **실험 1(가설 1 검증)**
    - 법규지식형 문제(A+B)에서 정답률과 법적 근거 일치도 비교(역 포템킨 이해 현상 관찰)
        - 법적 근거 일치도는 법률명과 조항까지 정확해야 함.
        - 출력형식에 대해 few-shot prompt 필요
        - answer matching를 활용하며 역시 few-shot prompt를 활용하고 예비적 실험 필요
    - 문제 유형간의 평균 정답률을 비교, 분석
- **실험 2: 추론 근거 기반 심층 분석 (가설 2 검증)**
    - 2a. : 법규지식형 객관식 문제와 이를 오픈형 또는 O/X문제로 변환한 문제의 정답율 및 법적 근거 일치도 비교
        - 오픈형문제의 정답율 및 법적 근거 일치도는 기 주어진 객관식 답변과 모델의 답변에 대해 다음 논문과 같이 answer matching 방법으로 확인하며 사전실험으로 사전 검증[Answer Matching Outperforms Multiple Choice for Language Model Evaluation](zotero://select/library/items/Z4WF5D4L)
    - 2b : 법규지식형 문제 중 단순 문장형와 사진/일러스트형 사례 문제에서 정답율과 법적 근거 일치도 비교. 방법은 2a와 동일
- **공통**
    - 실험 1, 2의 모든 문항에 대해, LLM이 답을 생성할 때 ‘정답- 이유- 법적 근거’ 구조로 답변하도록 하여 추론 근거를 확보한다.
    - 객관식: 선택지 기반 답변
    - 오픈형: Answer matching으로 정답 판정
    - 법적 근거: 해설의 법조문과 정확히 일치하는 경우만 인정

### 3. 분석

- 평가지표
    - ACC(정답율): 정답만을 비교했을 때 맞춘 비율
    - LRA (Legal Reason Accuracy) : 정답 + 법적 근거까지 맞춘 비율
    - FLR(Fake Legal Rate) : (정답율 - LRA)/  ACC → 평가 결과의 신뢰도(낮을수록 높은 신뢰도)
    - 최소 ACC 기준선 설정 또는 절대차이(ACC-LRA) 병행 사용
    - 해석 예시
        - 법규지식형에서 acc이 낮지만, 사례형에 비해 상대적으로 FLR이 낮다면 그나마 법규지식형이 사례형보다 법규지식에 대한 평가결과가 신뢰할만함
- 평가지표에 대한 해석
    - 한 문장 정의
        - **ACC**: "문제를 얼마나 잘 푸는가?"
        - **LRA**: "법을 얼마나 제대로 아는가?"
        - **FLR**: "정답 중 얼마나 많은 것이 운빨인가?"
    - 예시
        - **ACC = 90%**: "와, 90점이라니! 정말 똑똑하구나." (겉으로 보이는 성과)
        - **LRA = 45%**: "알고 보니, 제대로 알고 맞힌 건 전체의 45%뿐이네." (진짜 실력)
        - **FLR = 50% `((90-45)/90)`**: "맞힌 문제 중 절반(50%)은 엉터리 근거로 맞혔다는 뜻이군. 이 90점은 신뢰하기 어렵겠어." (거품 비율)
    - 혼동행렬
        - 네 가지 지식 상태
            
            
            |  | 법적근거 O | 법적근거 X |
            | --- | --- | --- |
            | **정답 O** | A1 (진정한 지식) | A2 (가짜 지식) |
            | **정답 X** | B1 (부분적 지식) | B2 (무지식) |
            1. **A1 영역**: "진짜 안다" - 개념도 알고 적용도 됨
            2. **A2 영역**: "포템킨의 역" - 적용은 되지만 개념 모름 ← **IPU 현상**
            3. **B1 영역**: "포템킨 원조" - 개념은 알지만 적용 안됨
            4. **B2 영역**: "모른다" - 개념도 모르고 적용도 안됨
    
    [평가지표에 대한 해석 - 세부](https://www.notion.so/230e0712d2858092a631e4eb51c3c475?pvs=21)
    
- 

## 5. Experimental Setup

### 1. 세부 실험 조건

- 문항 수 : 문장형, 사례형 각각 20문제씩 복원 추출하여 30회 실시(전체 문제는 문장형 500, 사례형 200문제)
- prompt
    
    ```
    다음은 운전면허 필기시험 문제입니다. 정답을 고르고, 선택 이유를 설명하고, 법적 판단에 근거한 것이라면 법령명을 명시하세요.
    
    문제:
    {문제 내용}
    
    선택지:
    A. {선택지1}
    B. {선택지2}
    C. {선택지3}
    D. {선택지4}
    
    [답변 양식]
    1. 정답: (선택지 번호)
    2. 이유: (서술형 설명 - 법적 조항 기반 또는 기타 설명)
    3. 법적 근거: (법령명 + 조항번호 명시, 없으면 '없음')
    
    ```
    
- 답변구조
    
    ```
    [답변 구조]
    1. 정답: (선택지 번호)
    2. 이유: (법적 조항 기반 or 상식 기반 설명)
    3. 판단 기준: (법적 조항 번호 or 판단 논리의 출처)
    
    [답변 구조]
    1. 정답: (선택지 번호)
    2. 이유: (서술형 - 법적 조항 기반 또는 기타 설명)
    3. 법적 근거: (법령명 + 조항번호 명시, 없으면 '없음')
    ```
    

## 6. 기대효과

본 연구는 단순히 정답률만으로 LLM의 법규 지식을 평가하는 것의 위험성을 명확히 경고하며, 자율주행, 경찰업무와 같이 안전이 중요한 분야에서는 반드시 **결과(what)와 과정(how)을 함께 검증하는 이원적 평가 체계**가 필요함을 강력히 시사한다. 본 연구의 결과는 향후 신뢰성 있는 AI 시스템을 위한 법규 및 규범 준수 능력 평가 방법론 개발에 중요한 학술적, 실용적 토대를 제공할 것이다.

이러한 연구 결과는 다음과 같은 학술적·실용적 기여(기대 효과)를 가질 것이다.

- **학술적 기여:** LLM 교통법규 지평가 분야에 역 포템킨 이해라는 구체적인 평가 함정을 실증적으로 제시하고, 결과(what) 중심의 평가에서 과정(how) 중심의 평가로의 패러다임 전환 필요성을 제기한다.
- **실용적 기여**
    - 자율주행 시스템의 안전성 검증을 위해 LLM의 법규 지식을 평가하는 더 정교하고 신뢰성 있는 방법론의 토대를 마련하며, 향후 관련 평가 체계 개발에 중요한 가이드라인을 제공할 수 있다.
    - 객관식 문제는 못풀지만 시각문제는 잘푸는 llm은 운전용으로는 적합할지 몰라도 경찰에게는 부족. 경찰이 llm을 법규지식용으로 사용하기 위한 평가에 고려해야 함

## 7. 추가 할일

1. 문제 분류 시 교차 검증
2. 문헌리뷰 : 
    1. 경찰업무, 자율주행에서 법규지식평가가 왜, 얼마나 중요한지
        - **분야별 중요성 연결:** 자율주행, 경찰 업무 외 의료, 금융 등 다른 **고위험(high-stakes) 분야**에서 LLM의 법규/규범 준수 능력 평가가 왜 중요한지에 대한 논의를 추가하여 연구의 중요성과 확장 가능성을 강조할 수 있습니다.
    2. **평가 함정의 계보**
        - '역 포템킨 이해'나 '객관식 지름길'과 유사한 LLM의 평가 함정을 다룬 기존 연구들(예: shortcut learning, surface-level heuristics, prompt sensitivity 등)을 폭넓게 고찰하고, 이들과 본 연구가 제안하는 개념의 차별점을 명확히 부각합니다.
3. 한계 정리
    - 다른 잠재적 발현 양태
        - **Statistical shortcuts**: 객관식 지름길 (본 연구)
        - **Common-sense reasoning**: 일상 경험 기반 추론 (후속 연구)
        - **Prompt engineering artifacts**: 프롬프트 구조 의존성 (후속 연구)
    - 본 연구는 LLM이 '명시적 법적 지식' 없이 문제를 해결하는 현상에 초점을 맞춘다. 암묵적 지식, 절차적 지식 등 다른 형태의 지식 활용에 대한 분석은 후속 연구에서 다룰 예정이다.
4. (교통)법규 지식 도메인의 차별화지점
    - **Binary nature**: 법은 맞다/틀리다가 명확함
    - **Safety-critical**: 실수의 결과가 치명적
    - **Common-sense overlap**: 일상 상식과 겹치는 부분이 많음
5. 질적 분석(아이디어 차원임)
    1. 목적 : 분석대상을 선별해 역프템킨 이해 현상의 구체적 발현 형태 예
        1. **2단계: 흥미로운 분석 대상 선별**
        
        이제 전체 데이터가 아닌, 다음과 같이 의미 있는 특정 케이스들을 '저격'하여 선별합니다.
        
        - **정답은 맞혔으나, 근거는 틀린 경우 (ACC=1, LRA=0)**: '역 포템킨 이해' 현상을 가장 명확하게 보여주는 대표적인 사례들입니다. 이 답변들을 집중적으로 분석합니다.
        - **모델 간 성능이 엇갈리는 경우**: 똑같은 문제에 대해 A 모델은 맞혔는데 B 모델은 틀린 경우, 혹은 그 반대의 경우를 모두 추출합니다. 두 모델의 답변과 추론 과정을 비교하면 각 모델의 강점과 약점, 고유한 오류 패턴을 발견할 수 있습니다.
        - **모든 모델이 공통적으로 틀린 경우**: 모든 SOTA 모델을 실패하게 만든 '킬러 문항'들입니다. 이 문제들이 왜 그렇게 어려운지 분석하는 것만으로도 큰 의미가 있습니다.
        - **환각이 발생한 경우 (HLR > 0)**: 존재하지 않는 법 조항을 만들어낸 답변들을 모아 어떤 식으로 환각이 발생하는지 유형을 분석합니다
    2. **3단계: 선별된 데이터에 대한 '오류 유형 분석(Error-Type Analysis)'**
        
        위에서 선별한 소수의 데이터에 대해서만, 어떤 종류의 오류를 범했는지 분류합니다. 예를 들어 다음과 같은 '오류 태그'를 만들 수 있습니다.
        
    - **오류 유형 1: 상식 의존 오류 (Common-Sense Error)**: 법규가 아닌, "보통은 이렇게 운전하니까"와 같은 일반 상식에 기반해 추론한 경우.
    - **오류 유형 2: 법규 오적용 오류 (Misapplication Error)**: 법 조항을 언급하긴 했으나, 해당 상황에 잘못 적용하거나 내용을 잘못 해석한 경우.
    - **오류 유형 3: 사실관계 오인 오류 (Factual Error)**: 문제에 제시된 상황 자체(예: 그림, 영상)를 잘못 이해한 경우.
    - **오류 유형 4: 비논리적 추론 오류 (Logical Error)**: 제시한 근거와 결론 사이에 논리적 연결고리가 없는 경우.
6. power analysis